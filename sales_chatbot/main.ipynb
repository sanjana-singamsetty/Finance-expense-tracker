{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaning completed. Cleaned file saved as: sales_data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"./sales_data/Sales_Data.csv\"  # Change this to the actual file path\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Strip spaces from column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Step 2: Convert 'Order Date' to datetime format\n",
    "df[\"Order Date\"] = pd.to_datetime(df[\"Order Date\"], errors=\"coerce\")\n",
    "\n",
    "# Step 3: Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 4: Convert 'Quantity Ordered' and 'Price Each' to numeric types\n",
    "df[\"Quantity Ordered\"] = pd.to_numeric(df[\"Quantity Ordered\"], errors=\"coerce\")\n",
    "df[\"Price Each\"] = pd.to_numeric(df[\"Price Each\"], errors=\"coerce\")\n",
    "\n",
    "# Step 5: Verify Sales calculation\n",
    "df[\"Sales\"] = df[\"Quantity Ordered\"] * df[\"Price Each\"]\n",
    "\n",
    "# Step 6: Extract city names from 'Purchase Address'\n",
    "df[\"City\"] = df[\"Purchase Address\"].apply(lambda x: x.split(\",\")[1].strip())\n",
    "\n",
    "# Step 7: Extract hour from 'Order Date' column\n",
    "df[\"Hour\"] = df[\"Order Date\"].dt.hour\n",
    "\n",
    "# Save the cleaned data\n",
    "cleaned_file_path = \"sales_data_cleaned.csv\"\n",
    "df.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(\"Data cleaning completed. Cleaned file saved as:\", cleaned_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete. Processed data saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_csv(\"./sales_data/sales_data_cleaned.csv\")\n",
    "\n",
    "# Feature selection: Using 'Quantity Ordered', 'Price Each', 'Hour' to predict 'Sales'\n",
    "X = df[[\"Quantity Ordered\", \"Price Each\", \"Hour\"]]\n",
    "y = df[\"Sales\"]\n",
    "\n",
    "# Splitting data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save processed data\n",
    "with open(\"processed_data.pkl\", \"wb\") as data_file:\n",
    "    pickle.dump((X_train, X_test, y_train, y_test), data_file)\n",
    "\n",
    "print(\"Data preparation complete. Processed data saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete. Model and scaler saved in 'Trained_models' folder successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a folder if it doesn't exist\n",
    "folder_name = \"Trained_models\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"processed_data.pkl\", \"rb\") as data_file:\n",
    "    X_train, X_test, y_train, y_test = pickle.load(data_file)\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Train Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Save the model and scaler in the created folder\n",
    "with open(os.path.join(folder_name, \"lr_model.pkl\"), \"wb\") as model_file:\n",
    "    pickle.dump(lr_model, model_file)\n",
    "\n",
    "with open(os.path.join(folder_name, \"scaler.pkl\"), \"wb\") as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "print(f\"Model training complete. Model and scaler saved in '{folder_name}' folder successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.26%\n",
      "Precision: 97.09%\n",
      "Recall: 100.00%\n",
      "F1-score: 98.52%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"processed_data.pkl\", \"rb\") as data_file:\n",
    "    _, X_test, _, y_test = pickle.load(data_file)\n",
    "\n",
    "# Load the trained model and scaler\n",
    "with open(\"./Trained_models/lr_model.pkl\", \"rb\") as model_file:\n",
    "    lr_model = pickle.load(model_file)\n",
    "\n",
    "with open(\"./Trained_models/scaler.pkl\", \"rb\") as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "# Standardize the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert sales into binary classes (high = 1, low = 0) using the median as a threshold\n",
    "threshold = y_test.median()\n",
    "y_test_class = (y_test >= threshold).astype(int)\n",
    "y_pred_class = (y_pred_lr >= threshold).astype(int)\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class) * 100\n",
    "precision = precision_score(y_test_class, y_pred_class) * 100\n",
    "recall = recall_score(y_test_class, y_pred_class) * 100\n",
    "f1 = f1_score(y_test_class, y_pred_class) * 100\n",
    "\n",
    "# Print only Accuracy, Precision, Recall, and F1-score\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1-score: {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model trained and saved successfully to Trained_models/rf_model.pkl.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "folder_name = \"Trained_models\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"processed_data.pkl\", \"rb\") as data_file:\n",
    "    X_train, _, y_train, _ = pickle.load(data_file)\n",
    "\n",
    "# Train Random Forest Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(folder_name, \"rf_model.pkl\")\n",
    "with open(model_path, \"wb\") as model_file:\n",
    "    pickle.dump(rf_model, model_file)\n",
    "print(f\"Random Forest model trained and saved successfully to {model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"processed_data.pkl\", \"rb\") as data_file:\n",
    "    _, X_test, _, y_test = pickle.load(data_file)\n",
    "\n",
    "# Load the trained Random Forest model\n",
    "with open(\"./Trained_models/rf_model.pkl\", \"rb\") as model_file:\n",
    "    rf_model = pickle.load(model_file)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Convert sales into binary classes (high = 1, low = 0) using the median as a threshold\n",
    "threshold = y_test.median()\n",
    "y_test_class = (y_test >= threshold).astype(int)\n",
    "y_pred_class = (y_pred_rf >= threshold).astype(int)\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class) * 100\n",
    "precision = precision_score(y_test_class, y_pred_class) * 100\n",
    "recall = recall_score(y_test_class, y_pred_class) * 100\n",
    "f1 = f1_score(y_test_class, y_pred_class) * 100\n",
    "\n",
    "# Print only Accuracy, Precision, Recall, and F1-score\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1-score: {f1:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree model trained and saved successfully to Trained_models/dt_model.pkl.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "folder_name = \"Trained_models\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"processed_data.pkl\", \"rb\") as data_file:\n",
    "    X_train, _, y_train, _ = pickle.load(data_file)\n",
    "\n",
    "# Train Decision Tree Model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)  # You can add hyperparameters here\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = os.path.join(folder_name, \"dt_model.pkl\")\n",
    "with open(model_path, \"wb\") as model_file:\n",
    "    pickle.dump(dt_model, model_file)\n",
    "print(f\"Decision Tree model trained and saved successfully to {model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Precision: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load preprocessed data\n",
    "with open(\"processed_data.pkl\", \"rb\") as data_file:\n",
    "    _, X_test, _, y_test = pickle.load(data_file)\n",
    "\n",
    "# Load the trained Decision Tree model\n",
    "with open(\"./Trained_models/dt_model.pkl\", \"rb\") as model_file:\n",
    "    dt_model = pickle.load(model_file)\n",
    "\n",
    "# Predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Convert sales into binary classes (high = 1, low = 0) using the median as a threshold\n",
    "threshold = y_test.median()\n",
    "y_test_class = (y_test >= threshold).astype(int)\n",
    "y_pred_class = (y_pred_dt >= threshold).astype(int)  # Use Decision Tree predictions\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class) * 100\n",
    "precision = precision_score(y_test_class, y_pred_class) * 100\n",
    "recall = recall_score(y_test_class, y_pred_class) * 100\n",
    "f1 = f1_score(y_test_class, y_pred_class) * 100\n",
    "\n",
    "# Print only Accuracy, Precision, Recall, and F1-score\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Precision: {precision:.2f}%\")\n",
    "print(f\"Recall: {recall:.2f}%\")\n",
    "print(f\"F1-score: {f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame, replace with your actual data\n",
    "file_path = \"./sales_data/Sales_Data.csv\" \n",
    "df = pd.read_csv(file_path)  # Replace with the correct data loading\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = df[[\"Quantity Ordered\", \"Price Each\", \"Hour\"]]  # Example features\n",
    "y = df[\"Sales\"]  # Example label, replace with your actual label column\n",
    "\n",
    "# Save the preprocessed data\n",
    "with open(\"processed_data.pkl\", \"wb\") as data_file:\n",
    "    pickle.dump((X, None, y, None), data_file)\n",
    "\n",
    "# Now, this processed_data.pkl can be loaded again later and will contain X_test and y_test properly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (185950, 3)\n",
      "y_test length: 185950\n",
      "Response: The text indicates the result of an analysis, specifically a prediction yielding a numerical value of 185.21274596665765.  Without more context (e.g., what was being analyzed, what method was used for prediction), it's impossible to say what the number *means*.  It could be anything from a predicted stock price to the estimated weight of something to the calculated outcome of a scientific experiment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Load the trained Decision Tree model directly from the 'Trained_models' folder\n",
    "folder_name = \"Trained_models\"\n",
    "model_path = os.path.join(folder_name, \"lr_model.pkl\")\n",
    "with open(model_path, \"rb\") as model_file:\n",
    "    dt_model = pickle.load(model_file)\n",
    "\n",
    "# Load your preprocessed data for prediction\n",
    "with open(\"processed_data.pkl\", \"rb\") as data_file:\n",
    "    X_test, _, y_test, _ = pickle.load(data_file)\n",
    "\n",
    "# Check if the data was loaded correctly\n",
    "print(f\"X_test shape: {getattr(X_test, 'shape', 'No shape')}\")\n",
    "print(f\"y_test length: {len(y_test)}\")\n",
    "\n",
    "# Set up the Gemini API URL and API key\n",
    "GEMINI_API_URL = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent\"\n",
    "API_KEY = \"AIzaSyDs_6CZdiVLoBaZZltYrQC5FiGgf9gXGfU\"  # Replace with your actual API key\n",
    "\n",
    "# Function to get the response from Gemini API\n",
    "def chatbot_response(user_text, model, X_test, y_test):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Here, we're assuming the user_input correlates to features in X_test\n",
    "        # You might need to map user input to features before using it for prediction\n",
    "        user_features = extract_features_from_text(user_text)  # Implement this function based on your needs\n",
    "        \n",
    "        # Predict using the decision tree model\n",
    "        prediction = model.predict([user_features])[0]\n",
    "\n",
    "        # Prepare context for the chatbot to generate response\n",
    "        context = f\"Based on the analysis, the predicted result is: {prediction}\"\n",
    "\n",
    "        # Prepare the data for the Gemini API\n",
    "        modified_data = {\n",
    "            \"contents\": [{\n",
    "                \"parts\": [{\"text\": f\"{user_text}. Context: {context}\"}]\n",
    "            }]\n",
    "        }\n",
    "\n",
    "        # Make the POST request to the Gemini API\n",
    "        response = requests.post(f\"{GEMINI_API_URL}?key={API_KEY}\", headers=headers, data=json.dumps(modified_data))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            try:\n",
    "                answer = response.json().get(\"candidates\", [{}])[0].get(\"content\", {}).get(\"parts\", [{}])[0].get(\"text\", \"Sorry, I couldn't find an answer.\")\n",
    "                return answer\n",
    "            except (KeyError, IndexError, TypeError):\n",
    "                return \"Invalid response format from Gemini API.\"\n",
    "        else:\n",
    "            return f\"Error with Gemini API: {response.text}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing input: {e}\"\n",
    "\n",
    "# Placeholder function to extract features from text (replace with actual feature extraction logic)\n",
    "def extract_features_from_text(user_text):\n",
    "    # Here, you'd convert the user input into numeric features that match the model's input (X_test).\n",
    "    # For now, this just returns a random feature (you'll need to replace this).\n",
    "    return [0] * X_test.shape[1]  # Placeholder (should match the number of features in X_test)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    user_input = \"what is this text about\"  # Replace with any user input\n",
    "    response = chatbot_response(user_input, dt_model, X_test, y_test)\n",
    "    print(f\"Response: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
